%
%  Copyright (c) CERN 2011
%
%  Copyright (c) Members of the EMI Collaboration. 2011-2013
%  See  http://www.eu-emi.eu/partners for details on the copyright
%  holders.
%
%  Licensed under the Apache License, Version 2.0
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Simple Sectioned Essay Template
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing essay content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt, it can be changed here

\usepackage{geometry} % Required to change the page size to A4
\geometry{a4paper} % Set the page size to be A4 as opposed to the default US Letter

\usepackage{graphicx} % Required for including pictures
\usepackage{listings}
\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{wrapfig} % Allows in-line images such as the example fish picture
\usepackage{color}
\linespread{1.2} % Line spacing

%\setlength\parindent{0pt} % Uncomment to remove all indentation from paragraphs

\graphicspath{{./Pictures/}} % Specifies the directory where pictures are stored




\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=perl,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true,
  breaklines=true,
  frame=none
}
%%  numberstyle=\tiny\color{gray},
%%  keywordstyle=\color{blue},
%%  commentstyle=\color{dkgreen},
%%  stringstyle=\color{mauve},


\begin{document}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

\textsc{\LARGE CERN IT}\\[1.5cm] % Name of your university/college
%\textsc{\Large Major Heading}\\[0.5cm] % Major heading such as course name
%\textsc{\large Minor Heading}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Dynafed v1.4.0}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Fabrizio \textsc{Furano} \\ % Your name\\
\end{flushleft}
\end{minipage}
%~
%\begin{minipage}{0.4\textwidth}
%\begin{flushright} \large
%\emph{Supervisor:} \\
%Dr. James \textsc{Smith} % Supervisor's Name
%\end{flushright}
%\end{minipage}\\[4cm]

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package



\vfill % Fill the rest of the page with whitespace

\end{titlepage}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents % Include a table of contents

\newpage % Begins the essay on a new page instead of on the same page as the table of contents 





\begin{abstract}
A number of storage elements and Grid data management components offer standard protocol interfaces like WebDAV for access to their data repositories. Here we report on work that seeks to exploit the federation potential of these protocols and build a system that offers a unique view of the storage and metadata ensemble and the possibility of integration of other compatible resources such as those from cloud providers.
The challenge, here undertaken by the providers of dCache and DPM, and pragmatically open to other Grid and Cloud storage solutions, is to build such a system while being able to accommodate  name translations from existing catalogues (e.g. LFCs), experiment-based metadata catalogues, or stateless algorithmic name translations, also known as ''trivial file catalogues''.
Such so-called storage federations of standard protocols-based storage elements give a unique view of their content, thus promoting simplicity in accessing the data they contain and offering new possibilities for resilience and data placement strategies.
The goal is to consider storage elements and metadata catalogues that can be queried through HTTP and WebDAV dialects (thus including S3 and MS Azure) and make them able to cooperate through an architecture that properly feeds
the redirection mechanisms that they are based upon, thus giving the functionalities of a ''loosely coupled'' storage federation. One of the key requirements is to use standard clients (provided by OS'es or open source distributions, e.g. Web browsers) to access an already aggregated system; this approach is quite different from aggregating the repositories at the client side through some wrapper API, like for instance GFAL, or by developing new custom clients.
Other technical challenges that will determine the success of this initiative include performance, latency and scalability, and the ability to create worldwide storage federations that are able to redirect clients to repositories that they can efficiently access, for instance trying to choose the endpoints that are closer or applying other criteria.
We believe that the features of a loosely coupled federation of open-protocols-based storage elements will open many possibilities of evolving the current computing models without disrupting them, and, at the same time, will be able to operate with the existing infrastructures, follow their evolution path and add storage centers that can be acquired as a third-party service.
\end{abstract}






\section{Introduction}
In this document we describe the Storage federation system that we designed and built to match with the existing and upcoming Grid-related data management architectures.
The system is able to federate storage sites and metadata endpoints that expose a suitable data access protocol, into a transparent, high performance storage federation that exposes a unique name space. The architecture can accommodate LFN/PFN algorithmic name translations without the need of catalogues. On the other hand, if catalogues are needed, several of them can be accommodated into the same federation. The idea is to allow applications to access a globally distributed repository, to which sites participate. The applications would be able to efficiently access data that is spread through different sites, by means of a redirection mechanism that is supported by the data access protocol that is used. The focus is on “standard protocols” for data access, like HTTP and WebDAV, and NFS can be considered as well. The architecture and the components of such a system are anyway detached from the actual protocol that is used.\\
The focus of our design is on the fact that a federation may be composed by distant sites, and the redirection choices have to take this into account, without imposing the need of partitioning a federation into smaller ones on a geographical basis, or partitioning the name space.\\ Another point that is important for our design, is that such a system should be efficient also in the browsing case, e.g. allowing an user to list the content of a directory in a fast and reliable way that does not impact the performance of the whole system.





\section{The goal and the available components}

The purpose of the project is \textit{being able to aggregate storage and metadata farms exposing standard protocols that support redirections and WAN data access, making they behave as a unique system, building the illusion of a unique namespace from a set of distinct endpoints, being able to accommodate also explicit, catalogue-based indexing.} The more notable examples of suitable protocols are HTTP/WebDAV and NFS 4.1.


\begin{figure}
  \begin{center}
    \includegraphics[width=28pc]{diagram-principle.eps}
  \end{center}
  \caption{\label{fig_mergenamespace}How namespaces can be merged to federate three hosts.}
\end{figure}

Figure \ref{fig_mergenamespace} shows an example of the meaning of the term 'Federation' in this context. In the example we have three distributed storage systems, named \textit{A}, \textit{B} and \textit{C}, each one exposing a name space containing files and directories. The upper box in the picture shows how the federated name space looks like. The rule applied is "if a file or directory exists in at least one of the systems A, B or C, then it is considered \textit{as contained in the federated system}".

One more aspect of this aggregation of storage resources is that the aggregating system does not need to keep a persistent index of the content of the systems A+B+C. It can just query the subsystems when any information is needed, and eventually cache in some way the responses it receives.\\

Such a federation of sites has to be intended as a set of storage endpoints (and/or replica location catalogues or name translators) that is:
\begin{itemize}
\item protocol-homogeneous (a client that uses a particular protocol must be able to operate with all the members of the federation, using that protocol)
\item namespace-homogeneous (each file is identified by a unique string key, which we call path/name. If two storage endpoints have a file with the same name, then they are two replicas of the same file.)
\end{itemize}

Given the criteria the WLCG data is distributed through sites, we did not want to limit the design by forcing to create partitions of the data repositories based on the path/name. Although a formally correct idea, it's apparently not compatible with the way LHC data is being distributed and used.\\
 
 In order not to limit the use cases to file fetching, the repositories should support the protocol features that allow an application to analyze data via WAN (with a proper analysis application) \cite{xrdwan}. This is the case of the HTTP and, possibly, nfs4.1 protocols.\\

Although creating several, distinct federations is always possible, the most interesting case is obviously the one for which there is only one big efficient federation, with a logical unique entry point that eventually could be replicated in several places. For the purposes of our project it's immaterial if this big federation aggregates sub-federations or storage endpoints or name translators or replica location catalogues. As a comparison, the xrootd \cite{xrd} federations support this kind of very wide setups by means of a mechanism called \textit{peering}, which has a few constraints. We refer the reader to the xrootd documentation in the case they are not familiar with the concept.\\

The Grid software gives the possibility of choosing among several components the ones that are more suitable for handling the storage and metadata parts of the design of a computing model. Some of them are, for example DPM, LFC, dCache. Nowadays we have also to start considering as parts of the solution the upcoming evolutions of these components \cite{dpmfuture} \cite{dpmnew}, that are headed to supporting standard protocols like HTTP, WebDAV and NFS4.1 in the context of scientific computing. On top of this we must also consider the opportunity of acquiring storage as an external service, as an additional kind of endpoint to fit in a modern design.\\
The goal of the Dynamic Federations project is to give efficient tools that are able to accommodate in a coherent way the variety of storage and metadata endpoints that a distributed, heterogeneous deployment of storage farms will make available. At the same time, the guidelines of the project are to privilege the aspects that are related to performance, scalability and usability of the federation services.


\section{Some use cases}
We describe here a few use cases for our ''storage federation engine'', that we consider as clear examples of the features that our system provides and as deployment use cases that have been tested with the system we designed. These are not intended to be precise specifications of the system. Of course the list is not exhaustive, given the flexibility of the concept. Moreover, the various points do not exclude each other.\\

In the case of a big loosely coupled federation, the choices to redirect a client to a repository or another should be based at least on the availability of the requested resource in that endpoint. Other metrics can be considered, like the geographical location of the client with respect to the various possible servers, and/or the load of the endpoints.
In other words, a client in Switzerland should not be redirected to read data from Taipei, unless the requested data is hosted only in Taipei.\\

We would like to recall that we are treating data access protocols that natively support redirections. The idea is that an application would use only a ''standard'' client (like a Web browser or an application using an HTTP client) to access the data, without additional client-side software layers that emulate the aggregation of the storage centers.

\subsection{DPM and dCache via WebDAV}

Given a number of storage endpoints deploying the WebDAV door of the dCache or DPM systems \cite{dpmfuture} \cite{dpmnew}, a completely transparent federation of them is possible, using only features of the WebDAV protocol.
This use case has been the first one to be demoed by the Dynamic Federations project, and the first two endpoints that were added to a working federation have been a dCache instance at DESY (Germany) and a DPM instance in ASGC (Taipei). The test did what it advertised, i.e. the users could not realize that they were browsing and using a federation of two distant sites. Moreover, the feeling of performance that the system gives is the one of a site that is hosted in the federation's frontend machine, with a fast and smooth interactivity.

\subsection{Add third-party storage farms}

We cite here what was the first formulation of this use case to be fulfilled by the Dynamic Federations project: \\
\textit{We buy from a company a service consisting in 100PB of high performance storage, located in a remote server farm. The company only allows the use of the WebDAV/HTTP protocol to access it, since its technicians do not know anything else, and do not want to internally expose their infrastructure to unknown sophisticated systems, by installing them.\\
We want the clients to be able to see this service through the same entry point that aggregates other similar services in a completely transparent way, at least for the data reading case. We don't want the client applications to be instrumented in order to accommodate this case. We don't want a Data Management system to treat this case as an exception of some kind.\\
This would allow users to browse their files using Internet Explorer without potentially being aware of the location of the items they see, and to run their personal analyses pointing their applications to the unique entry point, using the URLs that they see in the browser.}\\

This use case may also accommodate the use case of the ''Cloud storage providers''. Technically, we chose to use a Cloud storage service provided by T-Mobile (Germany) through WebDAV, which then became a standard component of the various demos of the Dynamic Federations system. Recent improvements allow for example to insert seamlessly S3 storage endpoints.

\subsection{Create a small local federation of close sites sharing storage}

Having a flexible system that can manage storage federations opens many possibilities. One of them is being able to create a small federation of sites that share their storage, and appear as a unique storage element. In other words, a common repository (for example an instance of the so-called "conditions data" for a High Energy Physics experiment) may be distributed across collaborating sites. Doing so, the clients would not  need to know the exact location of the file they need, as they would just access it through the federation frontend.

\subsection{Add resources managed by one or more replica catalogues}

We informally define an \textit{LFC cloud} as a set of storage elements that contain file replicas that are indexed by an instance of an LFC file catalogue. In this example, without loss of generality, we suppose that the content of the LFC is accessible through an HTTP/DAV gateway, like the ones that have been recently released \cite{dpmfuture}.
Let’s suppose that we have two such clouds, as the example fits with no changes also the case in which there are more.\\

A ''storage federation engine'' acts as unique entry point for the two clouds, by hiding the fact that they are two. Hence, an HTTP client would contact the main federations frontend, and will have access to all the metadata from there, because the frontend machine aggregates and caches on the fly the results of the metadata queries that are forwarded to the endpoints.\\
If the client issues an HTTP GET request towards the frontend, this will simply redirect it to one of the endpoints that have just advertised the availability of the requested file.\\ In other words, the client can be redirected to the best endpoint in the most suitable LFC cloud, with a decision based on:
\begin{itemize}
\item availability of the file in the two clouds
\item possibly, proximity of the client with respect to all the available endpoints.
\end{itemize}
The interesting aspect in this example is that no additional indexing of the files is needed to federate the two LFC clouds, and their internal, local workflow can remain untouched. \\
Another worth mentioning point is that also connecting natively to an LFC database is supported by the architecture, as the corresponding plugin is available as well.
We believe that this kind of feature could be beneficial for Virtual Organisations whose content is managed by more than one instance of LFC, in different sites. A worldwide deployment would appear as an unique thing to applications that benefit from having access to the whole repository. At the same time, the local workflows and the ownership of the local catalogues would remain, also avoiding the network latency-related issues that may come from having only one catalogue service for a widely distributed VO.

\subsection{Federating file caches}

The fact that our Dynamic Federations system applies a dynamic behavior to the problem of federating storage and metadata endpoints opens the possibility of federating storage endpoints whose content may change at a faster pace with respect to a regular storage element.
This is the case, for example, of a storage cluster that acts as a file cache, hence files may appear and disappear, depending on the pattern of the file requests that it receives.\\
Such a storage federation that includes caches among its endpoints would have the benefits that come from both concepts:
\begin{itemize}
\item caches would provide their service to the site they belong to, fetching files from elsewhere and keeping them while they are being actively used
\item the same caches would advertise the files that they currently contain to the federation system.
\end{itemize}
The outcome of these two points is that the content of a file cache in a given moment could be used through the federation frontend by some other external client, or, eventually, by some similar file cache system that is trying to fetch the file. As a consequence, the federation frontend would have more endpoints to choose from when asked to redirect a client to a suitable server that hosts a file resource. This aspect, coupled with some other smart endpoint choosing criteria like e.g. geographic proximity, would represent a very relevant feature for a Grid-aware setup.
So far, work is foreseen to verify the usability, in the described context, of the Scalable Proxy Caches \cite{scalableproxycache}.


\subsection{Allow the system to apply geography-aware redirection choices}

A feature that we implemented in the system internally associates geographical coordinates and information to each replica that is known to the federation, by invoking a loadable "Geo" plugin. The same plugin can associate this information to each client request for locating a replica. As a consequence, the system can select the replica that is geographically closer to the client that requested it.\\
An easy implementation of the Geo plugin consisted in wrapping the MMDB API \cite{geoip}, that seems to provide a more than adequate level of performance (on the order of one million queries per second, as output by its internal tests). The result is that the Dynamic Federation system is able to redirect a client to the replica that is the closest to it, in a very efficient way.


\section{The system}

The Dynamic Federations system is built around a new internal component that is called Uniform Generic Redirector (UGR). Ugr exposes an API, called \textit{UgrConnector} that gives the functionalities of a namespace, thus including file/replica metadata information and directory listing information.

As visible in Figure \ref{fig_dynafeds1}, the Ugr acts as loader of a set of plugins, which interface it to the external storage and metadata endpoints. Each kind of plugin can talk to a different kind of external endpoint (one or many in principle).\\ Right now we developed the following plugins:

\begin{itemize}
\item A WebDAV/HTTP client plugin, that is able to talk to an external WebDAV/HTTP server endpoint. This plugin is based on an implementation of an advanced wrapper client interface (called DAVIX), built using \textit{libneon} \cite{libneon}. This component supports several kinds of authentication, and supports advanced metadata primitives (e.g. getting file listings with all the metadata information of the items, in a single transaction). 
\item A DMLite-client plugin, that is able to use an instance of DMLite as a source of metadata information. This allows using all the possibilities of integration offered by DMLite, for instance to connect natively to an LFC database or to an HDFS cluster.
\end{itemize}
All the plugins that have been developed so far privilege the internal parallelism, i.e. they are able to perform N tasks in parallel, where N is a parameter that needs to be tuned in order to find the right balance between the overall system performance and the load that can be put towards the endpoints.\\
The set of plugins that are loaded by an Ugr instance (and their configuration) is written in a configuration file. Each plugin can be loaded multiple times, with different parameters and prefix-based filename translations.\\

The typical use of the Ugr is to be loaded by some other frontend system, like for instance the DMLite library \cite{dpmnew}. In this form, Ugr acts as a DMLite plugin, taking full benefit of its architecture and behavior. DMLite is a pluggable, thin software layer that gives abstract functionalities of file catalogue and interface to storage pools. Thanks to its architecture, a very broad range of storage systems can be accessed, through suitable plugins. DMLite can also be plugged into an Apache server, thus accessing all of its features through WebDAV.\\
One of the consequences of this is that, through DMLite, plugging the Ugr into an Apache server becomes easily feasible, as shown in Figure \ref{fig_dynafeds1}.\\

Internally, the Ugr acts as a sophisticated handler of parallel requests for metadata information. The basic behavior on the trigger of a metadata query is as follows:
\begin{itemize}
\item if the query can be satisfied by the local in-memory namespace cache, just use the cache to compute the result. Otherwise:
\item trigger, in parallel, all the plugins by queueing the query into them, then wait for the result.
\item each plugin may internally decompose the query into subqueries (also parallel, depending on the plugin)
\item each plugin acts independently in order to satisfy the query and write its result into the namespace cache
\item when the gathered information is \textbf{sufficient} for that client to get the result, that client only is signalled so that it can get the desired information.
\item the plugins that eventually did not finish the processing just continue, eventually updating the content of the cache with the information they may still gather. 
\end{itemize}

This sequence of actions is performed for any client, in parallel, with no imposed limits to their concurrency inside Ugr.

In the next sections we show sequence diagrams that explain with some more details the internal behavior of such a system, in two relevant cases.

\begin{figure}
\begin{center}
\includegraphics[width=28pc]{dynafeds.eps}
\end{center}
\caption{\label{fig_dynafeds1}Exemplification of the system architecture.}
\end{figure}

\subsection{Two clients issue a stat() request}

In this example, shown in Figure \ref{fig_collab1} two clients want to know some metadata information about file X, e.g. its size. Hence, they invoke the API of the Ugr service and get the response. The response is constructed by the aggregation service on the base of the responses of the various plugins.\\

The clients get the response as soon as the service gets it from the endpoints that it aggregates.
A successive query for the same information gets a cached answer, as shown in the diagram.\\

Any primitive describing this behavior (e.g. getting the size of a file) could give the name to this operation, should the reader be uncomfortable with the choice of \textit{stat} of this example.\\

As previously said, this diagram wants to describe the basic internal interactions of such an aggregator service. The term Client refers here to any system that is able to invoke the API, single or multithreaded. As discussed, in the current deployments this client is an instance of DMLite embedded into an Apache server.

\begin{figure}
\begin{center}
\includegraphics[width=28pc]{collab1.eps}
\end{center}
\caption{\label{fig_collab1}Clients issuing a Stat() request for the same file.}
\end{figure}

\subsection{Two clients request the full list of the replicas of a file}

In this example, shown in Figure \ref{fig_collab1_loc}, two clients want to know the list of the locations of file X, eventually through name translations. Hence, they invoke the API of the Ugr service and get the response. The response is constructed by the aggregation service on the base of the responses of the various plugins.\\

Any primitive with this behavior (e.g. getting a list of replicas) could give the name to this operation, should the reader be uncomfortable with the choice of \textit{locateall} of this example. Some implementations refer to this functionality with \textit{getreplicas}. We wanted to use a different term in order to emphasize the fact that this specification is implementation-agnostic.\\

As previously said, this diagram wants to describe the basic internal interactions of such an aggregator service. The term Client refers here to any system that is able to invoke the API, single or multithreaded. In at least the DPM/LFC deployments this client will likely be an instance of the DMLite library \cite{dpmnew}.\\
 
The difference with respect to the previous case is that in order to discover all the replicas of a file the system has to wait for all the plugins to have finished. In the previous case instead, getting the size of a file just need the answer of the fastest of the endpoints.

 \begin{figure}
\begin{center}
\includegraphics[width=28pc]{collab1_locations.eps}
\end{center}
\caption{\label{fig_collab1_loc}Clients trying to discover all the locations of the same file.}
\end{figure}


\subsection{In-memory volatile namespace and caching}

Figure \ref{fig_cache} shows how the caching structure of UGR works, in order to cache the relevant parts of the federation's namespace.\\

UGR, in the default setup, is treated as a plugin of the Apache srver, hence it is subject to its behaviour, related to multiprocessing and multithreading. Normally, Apache spawns several processes, each one with its own thread pools that serve HTTP requests.\\

Each instance of UGR can do its local computations in a fast local workspace, whose internal behavior is similar to a cache. In order to put in relation the local workspaces of multiple processes, and optimize their behavior, UGR can connect to a memcached service, which must be big enough to cache a significant part of the working set of the storage metadata, i.e. the metadata items that are used more often by the clients.\\

In addition to that, a site hosting the frontend of a large federation may want to deploy more than one machine, under the same DNS name. In this case we may want to deploy a memcached service that is accessed by all the machines in the cluster, like the one shown in Figure \ref{fig_cache}.\\

The simple default configuration installs \textit{memcached} in the same server where Apache and UGR are installed.\\

\begin{figure}
  \begin{center}
    \includegraphics[width=36pc]{cache.eps}
  \end{center}
  \caption{\label{fig_cache}How threads, processes, servers and caches work.}
\end{figure}


\subsection{Simple name translations}

UGR embeds a configurable scheme of name translation, which has the purpose of creating a match between the path prefixes of the resulting global name space and of the individual name spaces of the endpoints whose content is aggregated.\\
The complete scheme is shown in Figure \ref{fig_n2n}. Starting from the left, the clients have the illusion of interacting with a name space that is the federation's namespace, e.g.\\

\lstinline{/myfed/dteam/mydir/myfile}\\
 
 which (in the case of a setup with HTTP) may come from an URL like:\\
 
 \lstinline"http://<host>/myfed/dteam/mydir/myfile" .\\\
 
 Then, internally, if the path starts with one of the prefixes specified in \textit{n2n\_pfx}, this prefix is stripped from the path and the \textit{n2n\_newpfx} is put in its place.
 
 The resulting path/filename is what is handled by the internal workspace, and propagated to the plugins.
 
  The plugin themselves can implement their own prefix-based name translation, which appends a prefix before contacting their endpoint.\\
  
 Setting up the name translations in order to minimize the length of the filenames in the internal workspace is a very good idea.\\





\begin{figure}
  \begin{center}
    \includegraphics[width=36pc]{n2n.eps}
  \end{center}
  \caption{\label{fig_n2n}How the default name translations work in order to federate storage metadata endpoints.}
\end{figure}

\section{Typical deployment of a Dynamic Federations frontend}

UGR is a very generic component that can be used in a variety of ways that are agnostic of the communication protocol that is used to talk to the clients. By \textit{typical deployment} we mean a description of the default configuration that comes when installing it from the official packages.\\
In this case, UGR is used to create storage federations based on the HTTP/WebDAV protocols, using Apache2 as frontend, and the DMLite library both as an adaptor between Apache and UGR, and as an optional way to aggregate other external metadata sources (like site-local instances of DPM and LFC).\\
Figure \ref{fig_fullconfig} shows how the default deployment of an UGR frontend looks like:

\begin{itemize}

	\item Apache2 is the frontend to the clients;
	\subitem \textit{/etc/ugr/zlcgdm-ugr-dav.conf} is the file that defines the virtual server that is fed through lcgdm\_dav and DMLite;
	\item lcgdm\_dav is the Apache module that is able to talk to DMLite, and to use it as a source of information;
	\item DMLite is a pluggable layer that provides storage element functionalities;
	\subitem \textit{/etc/ugr/ugrdmlite.conf} is a DMLite configuration file, which loads UGR as a source of metadata.
	\item UGR is loaded as DMLite plugin that provides name space information;
	\subitem \textit{/etc/ugr.conf} is the file that configures UGR.
	\item UGR has its internal high performance buffer caches, and is more efficient with an external memcached instance that synchronizes their content, acting as a 2nd level cache.
	\item The loaded plugins have their own configuration parameters. In the case of the HTTP/DAV plugin, the parameters are set in the same Ugr configuration file.
	
\end{itemize}

\begin{figure}
  \begin{center}
    \includegraphics[width=30pc]{fullconfig.eps}
  \end{center}
  \caption{\label{fig_fullconfig}Configuration structure of the typical deployment.}
\end{figure}

\section{Configuration parameters reference}
In this section, we document the UGR configuration directives and their usage. In the default deployment, these directives are contained in the \textit{/etc/ugr.conf} configuration file.

\subsection{Ugr core configuration}

\subsubsection{INCLUDE}

 Interpret as configuration files all the files that are contained in the give directory. Only absolute paths are accepted.\\
 
 Syntax:\\

\lstinline"INCLUDE: <path>"\\

\lstinline"<path>" is a directory containing Ugr configuration files.\\
The configuration files are loaded and processed by the Ugr configuration subsystem in ascending alphabetic order.\\

Example:\\
Load all the configuration files that are contained in \lstinline"/etc/ugr.conf.d/"\\
\lstinline"INCLUDE: /etc/ugr.conf.d/"

\subsubsection{glb.debug}

 Sets the UGR log verbosity.\\
 
 Syntax:\\

\lstinline"glb.debug: <level>"\\

\lstinline"<level>" is the desired debug level, from 0 to 10.\\ \\
 NOTE: UGR internally uses \lstinline"syslog" to log its activity, in the \lstinline"user" class. We refer to the \lstinline"syslog" documentation for the platform in use in order to configure its behavior, e.g. to output the log to a logfile.\\
 
 NOTE: a debug level higher than 1 severely affects the performance of UGR. Never set it to a value higher than 1 in a production server.\\



\subsubsection{glb.debug.components[]}

 Allows selecting the internal components that produce log lines. By default all the internal components produce log activity. The presence of this directive in the configuration file allows only the named components to produce log lines.\\
 Individual plugins are selectable under the name \lstinline"locplugin.<plugin_name>"
 Every \lstinline"glb.debug.components[]" line that appears in the configuration is considered as an individual component to enable log production for.

 Syntax:\\

\lstinline"glb.debug.components[]: <log_component>"\\

 Example:\\
\lstinline"glb.debug.components[]: <locplugin.MYSITE>"\\
\lstinline"glb.debug.components[]: <locplugin.MYOTHERSITE>"\\


\subsubsection{glb.addchildtoparentonput}
 When a federator handles the writing of a new file (PUT for HTTP), it normally tries to add the new item to the listing
of the parent directory. Setting \lstinline"glb.addchildtoparentonput" to \lstinline"false" disables this behavior, and may
reduce the CPU usage for applications that create many files.

 Syntax:\\

\lstinline"glb.addchildtoparentonput: <true | false>"\\

 Example:\\
\lstinline"glb.addchildtoparentonput: <false>"\\


\subsubsection{glb.addchildtoparentonstat}
 When a federator computes the stat() information for a file, it normally tries to add the new item to the listing
of the parent directory in the case it is already in the first level cache.\\
Setting \lstinline"glb.addchildtoparentonstat" to \lstinline"false" disables this behavior, and may reduce the CPU usage, especially for listing operations of large directories.

 Syntax:\\

\lstinline"glb.addchildtoparentonstat: <true | false>"\\

 Example:\\
\lstinline"glb.addchildtoparentonstat: <false>"\\

\subsubsection{glb.filepullhook}
 Specifies the full path to an executable (or executable script) that can pull a file from an external URL into one of the endpoints of the federation.\\
 
 Syntax:\\
 
 \lstinline"glb.filepullhook: <path to executable>"\\
 
 For more information on Third Party COPY requests, please see the section \ref{noteonTPC}.\\
 
\subsubsection{glb.filepushhook}
 Specifies the full path to an executable (or executable script) that can push a file from one of the endpoints of the federation towards an external URL .\\
 
 Syntax:\\
 
 \lstinline"glb.filepushhook: <path to executable>"\\
 
 For more information on Third Party COPY requests, please see the section \ref{noteonTPC}.\\
 
\subsubsection{glb.filepull.header2params[]}
 Map a given request header (in the case of HTTP requests) into one or more parameters that will be passed to a file pulling script.\\
 
 Syntax:\\
 
 \lstinline"glb.filepull.header2params[]: x-http-usefulheader"\\
 
  For more information on Third Party COPY requests, please see section \ref{noteonTPC}.\\
  
\subsubsection{glb.filepush.header2params[]}
 Map a given request header (in the case of HTTP requests) into one or more parameters that will be passed to a file pushing script.\\
 
 Syntax:\\
 
 \lstinline"glb.filepush.header2params[]: x-http-usefulheader"\\
 
  For more information on Third Party COPY requests, please see section \ref{noteonTPC}.\\
  


\subsubsection{\label{glb.locplugin}glb.locplugin}

 Load an UGR location plugin. This directive can be specified many times, thus loading several plugins.\\
 
 Syntax:\\

\lstinline"glb.locplugin[]: <path/name.so> <ID> <max_concurrency> [parameters]"\\

where:

\begin{itemize}
\item \lstinline"<path/name.so>" is the full path to the shared library that implements the plugin
\item \lstinline"<ID>" is a unique name that is given to this plugin instance. This is used to identify it in this configuration file, and give settings that are private to that loaded instance. These other settings have the syntax:\\
      \lstinline"locplugin.<ID>.<parm>: <value>"\\
     where <parm> is the plugin-specific parameter that we want to set.
\item \lstinline"<max_concurrency>" is the maximum number of threads that can enter the plugin at the same time
\item \lstinline"[parameters]" is a plugin-specific series of space-limited parameters
\end{itemize}


\subsubsection{glb.waittimeout}

The maximum time in seconds that a client can wait for some information to be collected by UGR. This is just a guard time. If reached, some external endpoint may be overloaded or not responsive. In this case, UGR may decide to internally treat it as disabled, until it becomes responsive again.
\\
Syntax:\\
\lstinline"glb.waittimeout: <number>"

Default value: 180

\subsubsection{glb.maxlistitems}
Mark as non listable any directory that contains more than a certain number of items. This avoids thrashing the cache in difficult situations. This setting deals only with listing, not with the metadata of the individual files or subdirectories that are needed by the clients. In other words, the items in the directory are always accessible, no matter how many they are.\\
Syntax:\\
\lstinline"glb.maxlistitems: <number>"


Default value: 10000

\subsubsection{glb.filterplugin}
A filter plugin is a generic way of filtering a list of replicas before it is given to the requesting client.
The typical usage example is to sort a list of replicas to be given to a client in ascending geographical distance from it.
\\
Syntax:\\
\lstinline"glb.filterplugin[]: <plugin path> <plugin parameters>"
\\
Example:\\
Sort all the replicas in ascending geographical distance from the client.
\lstinline"glb.filterplugin[]: libugrgeoplugin_geoip.so geoplug1 /usr/share/GeoIP/GeoLiteCity.dat"

\subsection{\label{globalxlation}Global name translation}

UGR can process the file paths it is requested, in order to normalize them by means of prefix substitutions. The typical case is to internally remove the prefix that is instead present in the requests.

\subsubsection{glb.n2n\_pfx}

\lstinline"glb.n2n_pfx" defines a list of prefixes which will be substituted by \lstinline"glb.n2n_newpfx" during name translation. If more than one prefix is defined then matches will be tried with the longest prefix first and the first match will be accepted.

Syntax:\\
\lstinline"glb.n2n_pfx: <string_prefix> [<string_prefix> ...]"

Default value: none
\\
Example:\\

\lstinline"glb.n2n_pfx: /myfed /myfed2"

If the requested path starts with \lstinline"/myfed" or \lstinline"/myfed2", remove this prefix, so that internally in UGR that file is known without prefix.

If the user requests /myfed/atlas/fabrizio/testfile.txt, internally the file will be known as /atlas/fabrizio/testfile.txt\\
If the user requests /myfed2/atlas/fabrizio/testfile.txt, internally the file will be known as /atlas/fabrizio/testfile.txt\\
The practical consequence of this example is that the federation will be browseable with the prefix ``myfed'' or the prefix ``myfed2'', like:\\
\lstinline"http://myfederation.myhost/myfed/<path>/<file>" or \\
\lstinline"http://myfederation.myhost/myfed2/<path>/<file>"

\subsubsection{glb.n2n\_newpfx}

After having eventually stripped out a prefix with the \lstinline"n2n_pfx" directive, replace it with a new prefix.\\
Syntax:\\
\lstinline"glb.n2n_newpfx: <string_prefix>"

Default value: none


\subsection{Slave plugins and replica translators}

A replica translator plugin is a plugin that contacts an endpoint as an external service in order to:

\begin{itemize}
 \item get file listing information to be merged into the UGR internal namespace
 \item given a logical file name, get a list of replicas that are known to this service, and ask other plugins for a realtime validation.
\end{itemize}

The typical usage of this behavior is to use an external service like the LFC (or any other similar service) as a replica name lookup.\\

What characterizes a plugin that is marked as a replica translator plugin is the fact that every replica that the service finds is proposed for validation to the plugins that are marked as slave. Replica translator plugins do not insert replicas into the internal name space, they delegate this action to the plugins that are marked as slave. These slave plugins have the responsibility of checking the entries and eventually insert them into the UGR name space. The most basic check that such a slave plugin may perform is to make sure that the endpoint is considered as online and working. More advanced checks may include actually verifying the presence of that file in that endpoint.\\

To mark a plugin as a replica translator the syntax is:

\lstinline"locplugin.<ID>.replicaxlator: true" (default: false)\\

To mark a plugin as a slave the syntax is:\\

\lstinline"locplugin.<ID>.slave: true" (default: false)\\




\subsection{Plugin-level filename prefix translation}

Every plugin can apply a private rule to translate filename prefixes from the endpoint's name space to the UGR name space. As an additional feature, this prefix translating rule may specify more than one prefix that can be recognized and translated to the internal UGR namespace.\\

When the xlatepfx directive is specified, the plugin will be triggered only if the incoming query matches one of the given endopint.

\subsubsection{Example: Translate SRM TURLs into HTTP URLs}
An advanced example for which this feature is used is to translate \lstinline"srm://" TURLS coming from external services like an LFC or a replica database. For the plugin being configured we want to specify that both backward-compatible srm syntaxes must be correctly recognised and translated.\\

Let's suppose that the UGR is asked for the file:\\

\lstinline"/myfed/atlas/data/file1.dat" \\

The global name translation of the UGR (described and configured like in \ref{globalxlation}) translates this file into the internal structure of the UGR namespace:\\

\lstinline"/atlas/data/file1.dat"\\

An HTTP location plugin is used as a master translation plugin. This queries an external service and returns a replica that may follow one of the two possible syntaxes for SRM TURLs.\\

\begin{lstlisting}
srm://host1.site.org/dpm/site.org/atlas/datadisk/xyzw/file1_replica1.dat
srm://host2.site.org:8446/srm/managerv2? SFN=/dpm/site.org/atlas/datadisk/xyzw/file1_replica1.dat
\end{lstlisting}

We want to map these two TURLs to the new HTTP-based service setup by the site. This service resides in the machine \lstinline"host3.site.org" that is different from the hosts that give the SRM service. In this service, we are told by the sysadmin that the path for the aforesaid replica is:\\

\lstinline"https://host3.site.org/bigstorage/atlas/datadisk/xyzw/file1_replica1.dat"\\




To do this, we have to:

\begin{itemize}
 \item configure a slave plugin named \lstinline"HOST3" that manages the endpoint \lstinline"host3.site.org", in order to check for the replicas indicated by the master translator plugin.
 \item configure the plugin-level filename prefix translation of this plugin, described in the following directive.
\end{itemize}


\subsubsection{locplugin.$<$ID$>$.pfxmultiply}
Search for a metadata item in multiple directory prefixes on the given plugin. The effect of this directive is to merge the content of several directories into one. An use case can be to see as merged the content of multiple directories representing \textit{space tokens} in a remote storage.\\

Syntax:\\
\lstinline{locplugin.<ID>.pfxmultiply: <prefix_1> [... <prefix_N>]}\\

Example:\\
This example says that this plugin has to consider as merged the content of 9 spacetoken directories in the given endpoint.\\
\begin{lstlisting}
locplugin.MYPLUGIN.pfxmultiply: /disk-only/atlasgroupdisk/perf-tau /disk-only/atlasgroupdisk/perf-idtracking /disk-only/atlasgroupdisk/perf-egamma /disk-only/atlashotdisk /disk-only/atlasdatadisk /atlaslocalgroupdisk /disk-only/atlasproddisk /disk-only/atlasgroupdisk/soft-test /disk-only/atlasscratchdisk
\end{lstlisting}

\subsubsection{locplugin.$<$ID$>$.xlatepfx}
Sets up the path/name translation that the plugin identified by \lstinline"<ID>" can apply, in order to match the namespace of the endpoint it manages to the federation's namespace.\\
Simply speaking, this directive can be used to 'mount' a directory tree of the remote endpoint into the federation's namespace.\\

A side effect if this directory is that the plugin will be triggered only if the incoming query matches its namespace.\\

Another example tat makes use of this directive is to convert an SRM TURL got by some other service. In practice an SRM prefix has to be deleted and replaced with the HTTP URL of the remote HTTP endpoint.

Syntax:\\
\begin{lstlisting}
locplugin.<ID>.xlatepfx: <query_prefix_1> [... <query_prefix_N>] <prefix_to_substitute>
\end{lstlisting}
Where: \\

\lstinline"<query_prefix_1> [... <query_prefix_N>]" is a set of prefixes that will be matched against the incoming query. They may identify an SRM endpoint or just a part of the query string that has to be internally
removed in order to match the namespace of the remote endpoint. These prefixes, if found, will be internally stripped and substituted.\\

\lstinline"<prefix_to_substitute>" is the prefix that has to be put to match the namespace of the remote endpoint.\\

Example:\\

\lstinline"locplugin.HOST3.xlatepfx: /browseatlas /atlas/atlasdatadisk/rucio"\\

This example will show to the user a directory named \lstinline``browseatlas''. Browsing this directory will show the content gathered from \lstinline``/atlas/atlasdatadisk/rucio'' from the remote endpoint.

Example:\\
\begin{lstlisting}
locplugin.HOST3.xlatepfx: srm://host2.site.org:8446/srm/managerv2?SFN=/dpm/site.org/atlas/  srm://host2.site.org/dpm/site.org/atlas/ /
\end{lstlisting}

This example serves to federate a DPM endpoint as a slave replica checker.\\
A file named in DPM \\
\lstinline"srm://host1.site.org/dpm/site.org/atlas/datadisk/xyzw/file1_replica1.dat" would be known as \\
\lstinline"/datadisk/xyzw/file1_replica1.dat" in the internal UGR workspaces and caches.





\subsection{Infohandler advanced parameters}
The infohandler parameters influence how the internal UGR buffers collect  and reconstruct the information that comes from the endpoints, and act as a fast in-memory first level cache.



\subsubsection{infohandler.maxitems}
Set the maximum number of items in the cache.\\
Syntax:\\
\lstinline"infohandler.maxitems: <number>"\\

Default value: 1000

\subsubsection{infohandler.itemttl}
Set the maximum time (in seconds) an item is allowed to stay in the cache after it has been referenced.\\
Syntax:\\
\lstinline"infohandler.maxttl: <number>"\\
Default value: 600



\subsubsection{infohandler.itemmaxttl}
Set the maximum time (in seconds) an item is ever allowed to stay in the cache (even if it was referenced in the meantime).\\
Syntax:\\
\lstinline"infohandler.itemmaxttl: <number>"\\
Default value: 3600


\subsubsection{infohandler.itemttl\_negative}
Set the maximum time (in seconds) a negative information (like a "file not found" error) is allowed to stay in the cache.
Syntax:\\
\lstinline"infohandler.itemttl: <number>"\\
Default value: 10


\subsubsection{infohandler.useextcache}
If true, instantiate a 2nd level cache that uses \textit{memcached}. This cache will be shared among all the processes that are spawned.\\
This cache will also be used to store other information, like the status of the endpoints, whose determination can be shared among multiple processes running Ugr.
Syntax:\\
\lstinline"infohandler.useextcache: <true|false>"\\
Default value: true

\subsection{extcache.memcached.server[]}
In the case the \lstinline"infohandler.useextcache" is set to true, UGR will try to contact an external \textit{memcached} cluster.  This section describes the parameters that configure that.\\

 Syntax:\\

\lstinline"extcache.memcached.server[]: <IP_address>[:port]"\\

Add one memcached server to contact. Multiple entries like this add multiple servers to contact, in order to support advanced memcached clustering configurations.\\

Default value: 127.0.0.1:11211

\subsubsection{extcache.memcached.ttl}
Set the maximum time (in seconds) an item is allowed to live in the external memcached.
Syntax:\\
\lstinline"extcache.memcached.ttl: <number>"\\
Default value: 43200\\

\subsubsection{extcache.memcached.useBinaryProtocol}
Enable/disable the memcached binary protocol.
Syntax:\\
\lstinline"extcache.memcached.useBinaryProtocol: <true|false>"\\
Default value: true\\






\subsection{Authorization of groups and users}

When plugged into a DMLite setup (which is the normal way), Ugr can apply authorization schemes to the incoming requests.\\
In order to make this effective, the frontend (normally Apache) must be configured in order to apply a form of \textit{authentication} to the incoming clients. The Ugr \textit{authorization} scheme will
honour the user and groups information that is filled by the frontend. No restrictions are applied to the kind of authentication applied by the frontend. For Grid usage, this is normally X509 with
VOMS extensions, and also other Web-specific forms of authentication are supposed to work, e.g. Google, Facebook IDs, Federated Identities, etc. , provided that the frontend supports and correctly processes them.\\
Please note that the Ugr authorization is applied after any possible name groups or fqan translation that the frontend may apply. This is valid also for the gridmap translations that the sysadmin may want to configure in the system.\\
Authentication in Ugr is plugin-based. All the authorization plugins are executed sequentially for the same query, until one that grants access is found. If no plugin that grants access is found, then access is denied.\\
In the absence of additional authorization plugins to load, Ugr always has in memory a default implementation that implements a simple rule-based scheme.\\

\textbf{Important: the simple rule-based scheme always grants authorization if no rules are given. To allow any other plugin to deny authorization there must be at least one rule defined.}\\

What follows are the default authorization directives.\\

\subsubsection{glb.allowusers[]}
Add a rule that authorizes a specific user to read, write into, or list. Spaces in the username are supported only if the username is enclosed in double quotes.\\
The rule applies to all the URLs whose path matches the given prefix. Please note that the rule applies to the full URL, inclusive of path prefixes that other Ugr directives may mask or translate.\\
Syntax:\\
\lstinline"glb.allowusers[]: <username> <path prefix> [r][w][l][d][c]" \\

Parameters:\\
\begin{itemize}
 \item username : the username or DN to match this rule
 \item path prefix : the path prefix this rule applies to
 \item read "r" , write "w" , list "l" , delete "d", third-party COPY "c"
\end{itemize}

Example:\\
This example authorizes the anonymous user to get metadata, redirections and listings from the \textit{/fed/atlas} prefix.\\
\lstinline"glb.allowusers[]: nobody /fed/atlas rl" \\

\subsubsection{glb.allowgroups[]}
Add a rule that authorizes a specific group to read, write into, or list. Spaces in the username are supported only if the username is enclosed in double quotes.\\
If the groupname ends with \lstinline"*" then it is treated as a wildcard that will match all the group names or FQANs that start with the given string. As a consequence, the
string "*" will match with any group name or FQAN that the client may present, but will not match with an empty FQAN list.\\
If the groupname is an empty string enclosed in double quotes ( "" ) then the rule will match incoming requests that have an empty list of FQANs.\\
In any case, the rule applies to all the URLs whose path matches the given prefix. Please note that the rule applies to the full URL, inclusive of path prefixes that other Ugr directives may mask or translate.\\
Syntax:\\
\lstinline"glb.allowgroups[]: <groupname or fqan> <path prefix> [r][w][l][d][c]" \\

Parameters:\\
\begin{itemize}
 \item username : the username or DN to match this rule
 \item path prefix : the path prefix this rule applies to
 \item read "r" , write "w" , list "l" , delete "d", third-party COPY "c"
\end{itemize}

Example:\\
This example authorizes any user that is reported to belong to the 'atlas' group to get metadata, redirections and listings from
the \textit{/fed/atlas} prefix.\\
\lstinline"glb.allowgroups[]: atlas /fed/atlas rl" \\


\textbf{Important}:\\ All the auth plugins are checked for a given query, including the default one. If the default plugin has not to grant any authorization, then it has to
be explicitely configured to do so, by giving rules that always result in a denial. Example:\\
\begin{lstlisting}
glb.allowusers[]: nouser /nodir r
glb.authorizationplugin[]: libugrauthplugin_python26.so authplug1 ugrauth_example isallowed
\end{lstlisting}

\subsubsection{glb.authorizationplugin}
An authorization plugin is a plugin that implements some form of decision whether to authorize a request coming from a client.
\\
Syntax:\\
\lstinline"glb.authorizationplugin[]: <auth plugin library name> <plugin_name>  <Python module to import> <Python function to invoke>"
\\
Example:\\
To authorize a client, invoke the function \lstinline"isallowed()" from the python module \lstinline"ugrauth_example.py".
\lstinline"glb.authorizationplugin[]: libugrauthplugin_python26.so authplug1 ugrauth_example isallowed"




\subsubsection{The Python authorization plugin}
The Python authorization plugin invokes a function from a Python module to determine if a request coming from a client has to be authorized.\\
Please note that no script is executed or spawned, and the python function is invoked natively, using the python C API. This keeps performance high.\\

Syntax:\\
\lstinline"glb.authorizationplugin[]: libugrauthplugin_python26.so  <plugin_name>  <Python module to import> <Python function to invoke>"

Mandatory signature of the Python function:\\
\lstinline"def <any_func_name>(clientname="unknown", remoteaddr="noIP", resource="none", mode="0", fqans=None, keys=None)"\\

The module that is imported must be in a directory fulfilling one of these conditions:\\
\begin{itemize}
 \item it's a directory contained in the PYTHONPATH environment variable as evaluated within the Apache daemon
 \item the directory is \lstinline"/etc/ugr.conf.d/"
\end{itemize}
Both the directory and the Python module must be readable and executable by the user running the apache daemon.\\

Parameters:\\
\begin{itemize}
 \item clientname : the username or DN of the client
 \item remoteaddr : IP address of the client
 \item resource : the filename or directory name for which the operation has been requested
 \item mode : read "r" , write "w" , list "l" , delete "d", third-party COPY "c"
 \item fqans : list of groups or FQANS the user belongs to. VOMS roles will appear here.
 \item keys : list of other authentication keys that Apache has filled while authenticating the client.
\end{itemize}

The function's Return value is an integer. A zero value means that the access is granted. A nonzero value means that the access is denied.
A non callable function means that the access is denied.\\

Example:
To authorize a client, invoke the function \lstinline"isallowed()" from the python module \lstinline"ugrauth_example.py" that is available in the directory \lstinline"/etc/ugr.conf.d" .\\
\lstinline"glb.authorizationplugin[]: libugrauthplugin_python26.so authplug1 ugrauth_example isallowed"\\

Example Python module:\\

\begin{lstlisting}
#!/usr/bin/python
# -*- coding: utf-8 -*-

# Simple script that prints its arguments and then decides if the user has
# to be authorized
# usage:
# ugrauth_example.py clientname remoteaddr <fqan1> .. <fqanN>

import sys


def isallowed(clientname="unknown", remoteaddr="nowhere", resource="none", mode="0", fqans=None, keys=None):
    print "clientname", clientname
    print "remote address", remoteaddr
    print "fqans", fqans
    print "keys", keys
    print "resource", resource

    return 0


#------------------------------
if __name__ == "__main__":
    isallowed(sys.argv[1], sys.argv[2:])
"
\end{lstlisting}

\textbf{Warning: depending on the frontend (typically Apache) configuration, the python module may clash with other modules loaded by the frontend. A typical example is mod\_wsgi for Apache, which must be disabled for the ugr auth python module to work.
}

\subsection{Location Plugin}
The Location plugins (dmlite, lfc, DAV, HTTP, etc) have in common a group of parameters.

\subsubsection{locplugin.$<$ID$>$.listable}
Enable listing operations (e.g PROPFIND) on the collections referenced by the plugin.\\
Syntax:\\
\lstinline"locplugin.<ID>.readable: <true|false>"\\
Default value: true\\

\subsubsection{locplugin.$<$ID$>$.readable}
Enable read operations (e.g GET, HEAD, PROPFIND) on the resources referenced by the plugin.\\
Syntax:\\
\lstinline"locplugin.<ID>.readable: <true|false>"\\
Default value: true\\

\subsubsection{locplugin.$<$ID$>$.writable}
Enable write operations (e.g PUT) on the resources referenced by the plugin.\\
Syntax:\\
\lstinline"locplugin.<ID>.writable: <true|false>"\\
Default value: false\\

\subsubsection{locplugin.$<$ID$>$.max\_latency}
Define the maximum time that is allowed for an operation.\\
If max\_latency milliseconds are exceeded in any metadata operation, a warning will be printed in the log.\\
If max\_latency milliseconds are exceeded in a periodic check operation, the plugin will be put offline.\\

Syntax:\\
\lstinline"locplugin.<ID>.max_latency: <ms>"\\
Default value: 10000\\

\subsection{DAV and HTTP plugin}

The DAV plugin, when loaded as a location plugin instance, points to an external DAV or HTTP endpoint, to consider its content as part of the storage federation.\\

Syntax:\\
\lstinline"glb.locplugin[]: /usr/lib64/ugr/libugrlocplugin_dav.so <ID> <concurrency> <URL prefix>"\\
or
\lstinline"glb.locplugin[]: /usr/lib64/ugr/libugrlocplugin_http.so <ID> <concurrency> <URL prefix>"


The parameters \lstinline"<ID>" and \lstinline"<concurrency>" have already been described in the section \ref{glb.locplugin}

The parameter \lstinline"URL prefix" is the URL prefix that points to the endpoint to be federated. Both \lstinline"http://" and \lstinline"https://" are supported. Please note that if a prefix of the files stored in the external storage has to be ignored, then it can be added to this \lstinline"URL prefix". This is a basic way of implementing a simple algorithmic, prefix-based name translation that is similar to the concept of \textit{chroot jail} with respect to the \lstinline"sshd" configuration.\\

A typical example of this is the following:\\
\lstinline"glb.locplugin[]: /usr/lib64/ugr/libugrlocplugin_dav.so dav_plugin_dcache_desy 30 http://sligo.desy.de:2880/pnfs/desy.de/data"\\ \\

where the prefix \lstinline"/pnfs/desy.de/data" has to be stripped in order to consider the files as belonging to a uniform name space.\\

In other words, a file whose name inthe storage element is:\\
\lstinline"/pnfs/desy.de/data/atlas/fabrizio/testfile.txt"\\

will be known to UGR as\\
\lstinline"/atlas/fabrizio/testfile.txt"\\

The DAV plugin has several parameters that configure the way it works and authenticates with the servers. For all these parameters, \lstinline"<ID>" is the name that was assigned to the specific plugin instance we want to refer to.

\subsubsection{locplugin.$<$ID$>$.ssl\_check}
Enable or disable the SSL validity check of the remote host.\\
Syntax:\\
\lstinline"locplugin.<ID>.ssl_check: <true|false>"\\
Default value: true\\
\subsubsection{locplugin.$<$ID$>$.ca\_path}
Add a X509 certificate authorities directory, each CA file contained inside the directory will be considered as a valid CA for the plugin requests.\\
Syntax:\\
\lstinline"locplugin.<ID>.ca_path: <path/ca_dir>"\\
Default value: true\\
\subsubsection{locplugin.$<$ID$>$.cli\_type}
Type of the client side credential to use. The format type PEM, PROXY (VOMS) and PKCS12 are supported.
Syntax:\\
\lstinline"locplugin.<ID>.cli_type: (PEM|PKCS12|PROXY)"\\
Default value: PKCS12\\
\subsubsection{locplugin.$<$ID$>$.cli\_private\_key}
Path to a local file containing the client private key to use when contacting this endpoint. Only the PEM and proxy format are supported.\\
Syntax:\\
\lstinline"locplugin.<ID>.cli_private_key: <path/file>"\\
Default value: none\\
\subsubsection{locplugin.$<$ID$>$.cli\_certificate}
Path to a local file containing the client side credentials to use when contacting this endpoint. Only the PEM, proxy and PKCS12 format are supported.\\
Syntax:\\
\lstinline"locplugin.<ID>.cli_certificate: <path/file>"\\
Default value: none\\
\subsubsection{locplugin.$<$ID$>$.conn\_timeout}
TCP connection timeout (in seconds) to use when establishing a connection to this endpoint. \\
Syntax:\\
\lstinline"locplugin.<ID>.conn_timeout: <timeout>"\\
Default value: 15\\
\subsubsection{locplugin.$<$ID$>$.custom\_header[]}
Additional header fields to populate the requests with.
Syntax:\\
\lstinline"locplugin.<ID>.custom_header[]: <header line>"\\
Default value: none\\
Example: \lstinline"locplugin.<ID>.custom_header[]: X-Rucio-Auth-Token: blah123"\\
\subsubsection{locplugin.$<$ID$>$.ops\_timeout}
TCP communication timeout (in seconds) to use when sending/receiving data from this endpoint.\\
Syntax:\\
\lstinline"locplugin.<ID>.ops_timeout: <timeout>"\\
Default value: 60\\
\subsubsection{locplugin.$<$ID$>$.status\_checking}
Enable or disable the asynchronous endpoint status checker. If enabled, the endpoint will be probed at regular intervals of time.\\
Syntax:\\
\lstinline"locplugin.<ID>.status_checking: <true|false>"\\
Default value: true\\
\subsubsection{locplugin.$<$ID$>$.status\_checker\_frequency}
Set the frequency (in milliseconds) of the status checker.\\
Syntax:\\
\lstinline"locplugin.<ID>.status_checker_frequency: <time>"\\
Default value: 10000\\
\subsubsection{locplugin.$<$ID$>$.auth\_login}
Set the username to use in the case of endpoints that support user/pasword login.\\
Syntax:\\
\lstinline"locplugin.<ID>.auth_login: <username>"\\
Default value: none\\
\subsubsection{locplugin.$<$ID$>$.auth\_passwd}
Set the password to use in the case of endpoints that support user/password login.\\
Syntax:\\
\lstinline"locplugin.<ID>.auth_passwd: <password>"\\
Default value: none\\
\subsubsection{locplugin.$<$ID$>$.metalink\_support}
Enable or disable the metalink support for the plugin. \\
The metalink support give the possibility of the federation to query a list of replica of a given resource. \\
Metalink support allows for instance the construction of recursive federations.  \\
Syntax:\\
\lstinline"locplugin.<ID>.metalink_support: <true|false>"\\
Default value: false \\

\subsection{S3 plugin}
The S3 plugin is a location plugin allowing to include a bucket of Simple Storage Service Cloud Storage as part of the namespace. The S3 plugin supports read, write and list operations.

The S3 plugin supports the AWS credential delegation mechanism. The federation clients are redirected to the federated Cloud storage already authenticated via the AWS query string authentication mechanism.\\
The S3 plugin accepts the parameters of the HTTP plugin, plus a few that are related to the AWS signing process, listed here.\\

Syntax:\\
\lstinline"glb.locplugin[]: libugrlocplugin_s3.so <ID> <concurrency> <URL prefix>"\\

\subsubsection{locplugin.$<$ID$>$.s3.priv\_key}
Setup the AWS private key used for the S3 authentication.

Syntax:\\
\lstinline"locplugin.<ID>.s3.priv_key: <AWS secret key>"\\


\subsubsection{locplugin.$<$ID$>$.s3.pub\_key}
Setup the AWS access key used for the S3 authentication.

Syntax:\\
\lstinline"locplugin.<ID>.s3.pub_key: <AWS access key>"\\

\subsubsection{locplugin.$<$ID$>$.s3.region}
Sets the S3 region parameter that has to be used for the S3 signing algorithm version 4. Setting this parameter automatically
enables the S3 signing algorithm version 4, using the given S3 region parameter.

Syntax:\\
\lstinline"locplugin.<ID>.s3.region: <AWS region>"\\


\subsubsection{locplugin.$<$ID$>$.s3.signaturevalidity}
The S3 signatures computed by this plugin will expire after the given number of seconds. Default is one hour.\\

\textit{\textbf{A signed URL has to be requested to Dynafed in the moment it is used. Storing a signed URL or caching it in the client
is a questionable practice, at best.\\
The typical example for this is a batch job. As the queueing time for the job is not predictable, the job should
be given the plain HTTP(s) URL that has to be used against Dynafed by the job itself when it starts.\\}}


\textit{\textbf{Please note that the signature validity must be longer than the maximum TTL of entries in the
internal or external cache (the parameters infohandler.itemmaxttl and extcache.memcached.ttl)\\}}


Syntax:\\
\lstinline"locplugin.<ID>.s3.signaturevalidity: <number of seconds>"\\

\subsubsection{locplugin.$<$ID$>$.s3.alternate}
When using the default S3 signing algorithm (denominated v2 in the Amazon documentation), by default the URL prefix will have to be in the following format:

\lstinline"s3://<bucket_name>.s3.amazonaws.com"\\

where the bucket name appears in the hostname part of the URL.\\

Example:\\

\lstinline"s3://mybucket.s3.amazonaws.com"    or: \\
\lstinline"s3://mybucket.<any S3 service domain>"

 
If the boolean parameter \lstinline"alternate" is set to true, the Dynafed S3 plugin
will expect an alternate syntax for that URL, with the bucket name in the path:\\
 
\lstinline"s3://s3-<S3_region_name>.amazonaws.com/mybucket"    or: \\
\lstinline"s3://<any S3 service hostname>/mybucket"\\

Please consult your provider of Cloud storage to verify which URL format has to be used for the specific case. Amazon S3 recommends the default host-based one.\\

Syntax:\\
\lstinline"locplugin.<ID>.s3.alternate: <yes|true>"\\
Any value that is different from \lstinline"yes" or \lstinline"true" will be treated as a boolean value of false.\\


\subsubsection{A note on S3 multipart uploads}
S3 endpoints don't accept single PUT requests that are larger than a few GBs. The correct way for a client to upload a very large file into S3 is to follow the workflow
related to the "multipart uploads".\\
In the case of Dynafed and UGR, we can assume that the client does not have the S3 keys, hence it's UGR that has to provide all the pre-signed URLs that
are necessary for the client to go through the workflow.\\
\\
The workflow is as follows:
\begin{enumerate}
 \item Client sends generic PUT request for a large file to dynafed
 \item If an S3 backend is chosen for the upload, Dynafed will respond to the client with the usual redirection response,
 decorated with:
 \subitem a header line  \lstinline"x-ugrs3posturl" that contains the pre-signed URL needed to initiate the multipart upload (a POST request to the S3
 backend chosen by Ugr)
 \subitem a header line \lstinline"x-ugrpluginid" that identifies the Ugr plugin that has been chosen to receive the large upload
 \item The client detects the headers and realizes it has to initiate a multipart upload. A client that is unable to detect this will normally
 follow the signed redirection towards the S3 endpoint, which will likely not accept the upload of the large file.
 \item The client sends a POST request to the S3 backend, the response will contain the uploadid needed
 to upload the chunks.
 \item the client sends a PUT request to dynafed, specifying the headers \lstinline"x-s3-uploadid" coming from the S3 backend and the \lstinline"x-ugrpluginid"
 coming from Ugr, and will get back a list of presigned URLs that can be used to upload the various parts
 of the file. Tha last URL is needed to commit the content and finish the upload (POST).
\end{enumerate}
\textit{By default Ugr will produce a fixed number of pre-signed URLs. The client may ask for a specified number by providing the last PUT
request with a header line \lstinline"x-s3-upload-nchunks".}\\
For further information, the reader is encouraged to refer to the Amazon Web Services REST API documentation:\\
\lstinline"https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html"

\subsection{\label{noteonTPC}A note on third party copy requests}
Dynafed (depending on the used frontend, but here we assume Apache HTTPD) reacts to third party copy (TPC) requests by
executing an external helper script.\\
In the case of a \textit{pull file copy} (i.e. the copy of a file \textbf{towards} one of the federated endpoints) Ugr will invoke the script
indicated by the configuration directove \lstinline"glb.filepullhook", whose default value is \lstinline"/usr/bin/ugr-filepull".\\
In the case of a \textit{push file copy} (i.e. the copy of a file \textbf{from} one of the federated endpoints) Ugr will invoke the script
indicated by the configuration directove \lstinline"glb.filepushhook", whose default value is \lstinline"/usr/bin/ugr-filepush".\\
In both cases, the standard output of the script will be processed by Ugr looking for the \textit{GFAL/FTS performance markers}, whose
values will be forwarded to the client in the way that FTS expects.\\
What follows is the list of the parameters passed to both scripts:\\
\lstinline"/usr/bin/ugr-filepush cksumcheck cksumtype srcURL destURL x509proxypath auth <additional optional parameters>"\\
\lstinline"/usr/bin/ugr-filepull cksumcheck cksumtype srcURL destURL x509proxypath auth <additional optional parameters>"\\
where:\\
\begin{itemize}
 \item \lstinline"cksumcheck" is an integer, whose value is nonzero if the copy has to verify the checksums
 \item \lstinline"cksumtype" is a string identifying the checksum type that has to be used, e.g. \lstinline"adler32"
 \item \lstinline"srcURL" is the full URL of the source file
 \item \lstinline"destURL" is the full URL of the destination file
 \item \lstinline"x509proxypath" is a local path pointing to the x509 delegated proxy certificate of the user that requested the file copy
 \item \lstinline"auth" is additional authorization information. In the case of Apache HTTPD this field comes from the
 content of the \lstinline"Authentication" header of the original COPY request
 \item \lstinline"additional optional parameters" are filled by copying the value of selected HTTP
 headers. To activate this mechanism, the Ugr configuration must provide one or more directives
 \lstinline"glb.filepull.header2params[]" or \lstinline"glb.filepush.header2params[]", respectively for file push or pull requests.
\end{itemize}



\subsection{GeoIP plugin for geographical information (obsolete)}
The GeoIP plugin uses the GeoIP library produced by MaxMind \cite{geoip} in order to extract geographical information for:

\begin{itemize}
\item the remote client that is querying Dynafed
\item URLs of the remote replicas that Dynafed looks up
\end{itemize}


 This geographical information is used internally by Dynafed to choose the replica that is closest to the client, for redirection
purposes. The kind of database that works best for the purposes of Dynafed is denominated "GeoLite City". A free version can
be downloaded from the website \lstinline"http://www.maxmind.com"

Syntax:\\
\lstinline"glb.filterplugin[]: libugrgeoplugin_geoip.so geoplug1 <path to the GeoLiteCity DB>"\\

Example:\\
\lstinline"glb.filterplugin[]: libugrgeoplugin_geoip.so geoplug1 /usr/share/GeoIP/GeoLiteCity.dat"\\

 NOTE: as of 2017, MaxMind does not support GeoIP anymore. The GeoIP library is still available in the common Linux distributions and
 is usable (through this plugin) by anyone who owns one of the MaxMind's older database files. Please refer to Section \ref{MaxMindDB} for
 the newer library.

\subsubsection{glb.filterplugin.geoip.fuzz}
Tells to the GeoIP plugin to apply a fuzz value when sorting replicas according to their distance from the client. The unit of measure is kilometers, although some numerical approximation is to be expected.\\
This has the effect of load balancing sites that are within 'fuzz' kilometers from the client.\\


Syntax:\\
\lstinline"glb.filterplugin.mmdb.fuzz: <number>"
\\


Example:\\
Load balance among replicas that are within a distance range of 10Km from the client.\\
\lstinline"glb.filterplugin.mmdb.fuzz: 10"

\subsection{MaxMindDB plugin for geographical information}
\label{MaxMindDB}

The MaxMindDB plugin uses the MaxMindDB library produced by MaxMind \cite{geoip} in order to extract geographical information for:

\begin{itemize}
\item the remote client that is querying Dynafed
\item URLs of the remote replicas that Dynafed looks up
\end{itemize}


 This geographical information is used internally by Dynafed to choose the replica that is closest to the client, for redirection
purposes. The kind of database that works best for the purposes of Dynafed is denominated "GeoLite2 City". A free version can
be downloaded from the website \lstinline"http://www.maxmind.com"

Syntax:\\
\lstinline"glb.filterplugin[]: libugrgeoplugin_mmdb.so geoplug1 <path to the GeoLiteCity DB>"

Example:\\
\lstinline"glb.filterplugin[]: libugrgeoplugin_mmdb.so geoplug1 /usr/share/GeoIP/GeoLite2-City.mmdb"


\subsubsection{glb.filterplugin.mmdb.fuzz}
Tells to the GeoIP plugin to apply a fuzz value when sorting replicas according to their distance from the client. The unit of measure is kilometers, although some numerical approximation is to be expected.\\
This has the effect of load balancing sites that are within 'fuzz' kilometers from the client.\\


Syntax:\\
\lstinline"glb.filterplugin.mmdb.fuzz: <number>"
\\


Example:\\
Load balance among replicas that are within a distance range of 10Km from the client.\\
\lstinline"glb.filterplugin.mmdb.fuzz: 10"







\subsection{Microsoft Azure plugin}
The Azure plugin is a location plugin allowing to include a container of Microsoft Azure Storage as part of the namespace. The Azure plugin supports read,
write, delete and list operations.

\textbf{NOTE: Writing to an Azure-type endpoint needs a particular HTTP workflow to be able to upload files larger than 128MB. So far the DAVIX client supports it
natively, thus DAVIX can write files of any size to an Azure endpoint. The maximum file size that other common mainstream clients can upload to an Azure storage
may be limited to 128MB unless the client is used in a proper program or script that performs the operations that an Azure endpoint expects to upload a large file.
For reading instead, any HTTP mainstream client will work with no known limitations.}

The Azure plugin supports the Azure credential delegation mechanism. The federation clients are redirected to the federated Cloud storage already
authenticated via the Azure REST query string authentication mechanism.\\
The Azure plugin accepts the parameters of the HTTP plugin, plus a few that are related to the Azure signing process, listed here.\\

Syntax:\\
\lstinline"glb.locplugin[]: libugrlocplugin_azure.so <ID> <concurrency> <URL prefix>"\\

\subsubsection{locplugin.$<$ID$>$.azure.key}
Setup the Azure key used for the Azure REST authentication. This key can be found in the Microsoft Azure online console.

Syntax:\\
\lstinline"locplugin.<ID>.azure.key: <Azure key>"\\



\subsubsection{locplugin.$<$ID$>$.azure.signaturevalidity}
The Azure signatures computed by this plugin will expire after the given number of seconds. Default is one hour.\\

\textit{\textbf{A signed URL has to be requested to Dynafed in the moment it is used. Storing a signed URL or caching it in the client
is a questionable practice, at best.\\
The typical example for this is a batch job. As the queueing time for the job is not predictable, the job should
be given the plain HTTP(s) URL that has to be used against Dynafed by the job itself when it starts.\\}}

\textit{\textbf{Please note that the signature validity must be longer than the maximum TTL of entries in the internal or
external cache (the parameters infohandler.itemmaxttl and extcache.memcached.ttl)\\}}

Syntax:\\
\lstinline"locplugin.<ID>.azure.signaturevalidity: <number of seconds>"\\





\subsection{DAVrucio plugin}
The DAVrucio plugin, when loaded as a location plugin instance, points to an external DAV or HTTP endpoint, to consider its content as part of the storage federation.\\
It allows to browse and access a federation of remote sites that host content that is managed by the ATLAS experiment \textit{rucio} data management system.\\
The DAVrucio plugin works like the generic DAV plugin, and adds a directive that enables the hash-based name translation of the \textit{rucio} data management system.\\

Syntax:\\
\lstinline"glb.locplugin[]: /usr/lib64/ugr/libugrlocplugin_davrucio.so <ID> <concurrency> <URL prefix>"\\

In other words, the DAVrucio plugin will make the federation export two aliased namespaces, with the same file content accessible through different paths:
\begin{itemize}
 \item A human-browseable path obtained by just merging the namespaces of the remote endpoints.
 \item A non-browseable path that does not contain the rucio hashes after the rucio token. This path is more suitable for jobs.
\end{itemize}


\subsubsection{locplugin.$<$ID$>$.xlatepfx\_ruciohash}

Specifies the sets of prefixes that when matched will make the plugin apply the rucio hash-based name translation to it, in alternative to the regular xlatepfx translation.\\
Sets up the path/name translation that the plugin identified by \lstinline"<ID>" can apply, in order to match the namespace of the endpoint it manages to the federation's namespace.\\
Simply speaking, this directive can be used to 'mount' a directory tree of the remote endpoint into the federation's namespace.\\

It should be noted that the \textit{xlatepfx\_ruciohash} directive does not imply that a rucio \textit{scope} can be browsed in a flat way, without rucio hashes in the path.


Syntax:\\
\begin{lstlisting}
locplugin.<ID>.xlatepfx_ruciohash: <query_prefix_1> [... <query_prefix_N>] <prefix_to_substitute>
\end{lstlisting}

Default value: none\\

Example:\\
\begin{lstlisting}
glb.locplugin[]: /usr/local/lib64/ugr/libugrlocplugin_davrucio.so KIT 5 https://f01-060-110-e.gridka.de:2880/pnfs/gridka.de/atlas
locplugin.KIT.ssl_check: false
locplugin.KIT.cli_type:PROXY
locplugin.KIT.cli_certificate: /tmp/xyzproxy
locplugin.KIT.cli_private_key: /tmp/xyzproxy
locplugin.KIT.xlatepfx: /browseatlas /
locplugin.KIT.pfxmultiply: /disk-only/atlasgroupdisk/perf-tau /disk-only/atlasgroupdisk/perf-idtracking /disk-only/atlasgroupdisk/perf-egamma /disk-only/atlashotdisk /disk-only/atlasdatadisk /atlaslocalgroupdisk /disk-only/atlasproddisk /disk-only/atlasgroupdisk/soft-test /disk-only/atlasscratchdisk
locplugin.KIT.xlatepfx_ruciohash: /atlas /
\end{lstlisting}

This example configures an endpoint with the DAVrucio plugin. It has the following relevant features:

\begin{itemize}
 \item An user will be able to browse the content of the federation pointing the browser to \lstinline{<URL_prefix>/browseatlas/}. He will browse it as a regular storage federation containing this site. Hence, the rucio hashes will be visible and will give the ``hierarchical'' feeling.
 \item The content of all the specified ``spacetoken directories'' will be seen as merged.
 \item Any file will be reacheable with an URL like \\
       \lstinline{<URL_prefix>/browseatlas/rucio/data13_8TeV/11/b4/}\\
       \lstinline{log.01387999._000012.job.log.tgz.2} (click-on browser use case)
 \item Any file will be reachable ALSO with an URL like \\
       \lstinline{<URL_prefix>/atlas/rucio/data13_8TeV/}\\
       \lstinline{log.01387999._000012.job.log.tgz.2} (use case of a job accessing one file)
\end{itemize}




\subsection{DMLite client plugin}

The DMLite client plugin, when loaded as a location plugin instance, instantiates a DMLite instance as a source of metadata information to federate. The typical usage is to contact natively an LFC or DPM database, for increased performance.\\

Syntax:\\
\begin{lstlisting}
glb.locplugin[]: /usr/lib64/ugr/libugrlocplugin_dmliteclient.so <ID> <concurrency> <config>
\end{lstlisting}

The parameters \lstinline"<ID> <concurrency>" have already been described in the section \ref{glb.locplugin}

The parameter \lstinline"<config>" is the full path to a DMLite configuration file. To configure DMLite, we refer the reader to the DMLite documentation.



\section*{Acknowledgment}

This work was partially funded by the EMI project under European
Commission Grant Agreement INFSO-RI-261611



\begin{thebibliography}{9}

\bibitem{ibmglamour} IBM Glamour \lstinline"http://www.almaden.ibm.com/storagesystems/projects/glamour/". 


\bibitem{rfc5716} RFC 5716
\lstinline"http://tools.ietf.org/html/rfc5716"


\bibitem{gluster} Gluster
\lstinline"http://gluster.com"

\bibitem{glusterwiki} Gluster on Wikipedia
\lstinline"http://en.wikipedia.org/wiki/GlusterFS"


\bibitem{fedfs} FedFS
\lstinline"http://discolab.rutgers.edu/fs/"

\bibitem{xrdwan} Scalla/xrootd WAN globalization tools: Where we are
Fabrizio Furano and Andrew Hanushevsky 2010 J. Phys.: Conf. Ser. 219 072005
\lstinline"http://iopscience.iop.org/1742-6596/219/7/072005/"

\bibitem{xrd} The xrootd.org homepage
\lstinline"http://www.xrootd.org"

\bibitem{datamgmthep}
Furano F. Data Management in HEP: an approach. 
The European Physical Journal Plus
Volume 126, Number 1 (2011), 12, DOI: 10.1140/epjp/i2011-11012-2

\bibitem{dpmnew}Web enabled data management with DPM \& LFC
Alejandro Alvarez Ayllon, Alexandre Beche, Fabrizio Furano, Martin Hellmich, Oliver Keeble and Ricardo Brito Da Rocha
CHEP2012

\bibitem{dpmfuture}DPM: Future Proof Storage
Alejandro Alvarez, Alexandre Beche, Fabrizio Furano, Martin Hellmich, Oliver Keeble, Ricardo Rocha
CHEP2012

\bibitem{dpmcomp} DPM components
\lstinline"https://svnweb.cern.ch/trac/lcgdm/wiki/Dpm/Dev/Components"

\bibitem{geoip}
"This product includes GeoLite data created by MaxMind, available from http://www.maxmind.com/."

\bibitem{scalableproxycache}
Cristian Traian Cirstea
Grid Data Access: Proxy Caches and User Views
Eindhoven University of Technology
Stan Ackermans Institute / Software Technology
ISBN 978-90-444-1067-9


\bibitem{libneon}
\lstinline"http://www.webdav.org/neon/"

\end{thebibliography}



\end{document}